{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyannote.audio import Pipeline, Model, Inference\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift\n",
    "from sklearn.metrics import rand_score, adjusted_mutual_info_score, silhouette_score, davies_bouldin_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "directory  = \"audio_snimci\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_files_in_list(list_files: list, directory: str):\n",
    "  for f in os.listdir(directory):\n",
    "    if os.path.isfile(os.path.join(directory, f)):\n",
    "      list_files.append(os.path.join(directory, f))\n",
    "\n",
    "\n",
    "def function_heatmap(embedding: list):\n",
    "  \"\"\"speaker_1_list = []\n",
    "  for dir in list_dictionary:\n",
    "    insert_files_in_list(speaker_1_list,[],'a',dir)\"\"\"\n",
    "  n = len(embedding)\n",
    "  matrix = [[0]*n for _ in range(n)]\n",
    "  for i in range(n):\n",
    "    for j in range(n):\n",
    "      matrix[i][j] = cdist([embedding[i]],[embedding[j]],metric=\"cosine\")[0,0]\n",
    "  sns.heatmap(matrix)\n",
    "  return matrix\n",
    "def distribution(speaker_dictionaries,matrix):\n",
    "  previous_count = 0\n",
    "  cordinates_for_speaker = []\n",
    "  for speaker in speaker_dictionaries:\n",
    "    cordinates_for_speaker.append((previous_count,previous_count+len(os.listdir(speaker))))\n",
    "    previous_count+=len(os.listdir(speaker))\n",
    "  print(cordinates_for_speaker)\n",
    "  from_same_group = []\n",
    "  for speaker in cordinates_for_speaker:\n",
    "    l_cord = speaker[0]\n",
    "    r_cord = speaker[1]\n",
    "    for row in matrix[l_cord:r_cord]:\n",
    "      for e in row[l_cord:r_cord]:\n",
    "        from_same_group.append(e)\n",
    "\n",
    "  np_matrix = np.array(deepcopy(matrix))\n",
    "  for speaker in cordinates_for_speaker:\n",
    "    l_cord = speaker[0]\n",
    "    r_cord = speaker[1]\n",
    "    np_matrix[l_cord:r_cord,l_cord:r_cord] = 2\n",
    "  different_group = np_matrix.flatten()[np_matrix.flatten()!=2]\n",
    "\n",
    "  plt.hist(from_same_group,bins=20,color='green',alpha=0.5)\n",
    "  plt.hist(different_group,bins=10,color='red',alpha=0.5)\n",
    "  plt.show()\n",
    "\n",
    "def clusterisation_result(list_files_name: list, prediction: list):\n",
    "  p = {}\n",
    "  prediction_n = [ int(f) for f in prediction]\n",
    "  \n",
    "  for l in sorted(list(zip(list_files_name,prediction_n))):\n",
    "    if l[1] in p:\n",
    "      if not l[0] in p[l[1]]:\n",
    "        p[l[1]].append(l[0])\n",
    "    else:\n",
    "      p[l[1]] = []\n",
    "      p[l[1]].append(l[0])\n",
    "  for key, value in sorted(p.items()):\n",
    "      #print(f\"{key}: {[ v.split('/')[0] for v in value]}\")\n",
    "      print(f\"{key}: {[ v for v in value]}\")\n",
    "\n",
    "def k_means_fixed(list_files_new,embedding, no_cluster):\n",
    "  kmeans = KMeans(n_clusters=no_cluster)\n",
    "  kmeans.fit(embedding)\n",
    "  prediction_k_means_fixed = kmeans.labels_\n",
    "\n",
    "  clusterisation_result(list_files_new,prediction_k_means_fixed)\n",
    "def agglomerative(list_files_new,embedding, no_cluster):\n",
    "  print(\"Agglomerative method\")\n",
    "  model = AgglomerativeClustering(n_clusters=no_cluster,linkage='average')\n",
    "  prediction = model.fit_predict(embedding)\n",
    "\n",
    "  clusterisation_result(list_files_new,prediction)\n",
    "def mean_shift(list_files_new,embedding):\n",
    "  print(\"Mean Shift method\")\n",
    "  meanshift = MeanShift()\n",
    "  prediction = meanshift.fit_predict(embedding)\n",
    "\n",
    "  clusterisation_result(list_files_new,prediction)\n",
    "def wss(embedding):\n",
    "  WSS = []\n",
    "  for i in range(1,20):\n",
    "    kmeans = KMeans(n_clusters=i,random_state=0)\n",
    "    kmeans.fit(embedding)\n",
    "    WSS.append(kmeans.inertia_)\n",
    "  plt.plot(range(1,20),WSS)\n",
    "\n",
    "minimal_treshold = 0.4\n",
    "def calculate_threshold(i,embedding):\n",
    "  cosine_dist = sorted([ cdist([i],[j],metric='cosine')[0,0] for j in embedding])\n",
    "  difference = []\n",
    "  max_v = 0\n",
    "  index = 0\n",
    "  for i in range(1,len(cosine_dist)-1):\n",
    "    diff = cosine_dist[i+1]-cosine_dist[i]\n",
    "    difference.append(diff)\n",
    "    if diff>max_v and (cosine_dist[i]+cosine_dist[i+1])/2>minimal_treshold:\n",
    "      index = i\n",
    "      max_v = diff\n",
    "  treshold = (cosine_dist[index+1]+cosine_dist[index])/2\n",
    "  if treshold<0.8:\n",
    "    return treshold\n",
    "  return 0.8\n",
    "\n",
    "def my_algorithm_v2(list_files_new,embedding):\n",
    "  print(\"My algorithm\")\n",
    "  n = len(list_files_new)\n",
    "  prediction = [-1]*n\n",
    "  cluster=0\n",
    "  for i,audio_first in enumerate(embedding):\n",
    "    if prediction[i]>-1:\n",
    "      continue\n",
    "    treshold = calculate_threshold(audio_first,embedding)\n",
    "    for j,audio_second in enumerate(embedding):\n",
    "      if prediction[j]>-1:\n",
    "        continue\n",
    "      if cdist([audio_first],[audio_second],metric='cosine')[0,0]<treshold:\n",
    "        prediction[j] = cluster\n",
    "    cluster+=1\n",
    "\n",
    "  clusterisation_result(list_files_new, prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/4db4899737a38b2d618bbd74350915aa10293cb2/pytorch_model.bin`\n",
      "/home/lenovo/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/pyannote/audio/core/model.py:692: UserWarning: Model has been trained with a task-dependent loss function. Set 'strict' to False to load the model without its loss function and prevent this warning from appearing. \n",
      "  warnings.warn(msg)\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/4db4899737a38b2d618bbd74350915aa10293cb2/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.7.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.7.0+cu118. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['loss_func.W']\n"
     ]
    }
   ],
   "source": [
    "model = Model.from_pretrained(\"pyannote/embedding\",\n",
    "                              use_auth_token=token)\n",
    "inference = Inference(model, window=\"whole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = []\n",
    "\n",
    "insert_files_in_list(list_files,directory)\n",
    "embedding = [inference(file) for file in list_files]\n",
    "combined = list(zip(list_files,embedding))\n",
    "random.shuffle(combined)\n",
    "\n",
    "list_files_new, embedding_new = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_samples=4 should be >= n_clusters=5.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mwss\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_new\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mwss\u001b[39m\u001b[34m(embedding)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m,\u001b[32m20\u001b[39m):\n\u001b[32m     80\u001b[39m   kmeans = KMeans(n_clusters=i,random_state=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m   \u001b[43mkmeans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m   WSS.append(kmeans.inertia_)\n\u001b[32m     83\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m,\u001b[32m20\u001b[39m),WSS)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1464\u001b[39m, in \u001b[36mKMeans.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[32m   1429\u001b[39m \n\u001b[32m   1430\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1452\u001b[39m \u001b[33;03m    Fitted estimator.\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1454\u001b[39m X = validate_data(\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1456\u001b[39m     X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1461\u001b[39m     accept_large_sparse=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1462\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1466\u001b[39m random_state = check_random_state(\u001b[38;5;28mself\u001b[39m.random_state)\n\u001b[32m   1467\u001b[39m sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1404\u001b[39m, in \u001b[36mKMeans._check_params_vs_input\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_n_init\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1406\u001b[39m     \u001b[38;5;28mself\u001b[39m._algorithm = \u001b[38;5;28mself\u001b[39m.algorithm\n\u001b[32m   1407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._algorithm == \u001b[33m\"\u001b[39m\u001b[33melkan\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_clusters == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:871\u001b[39m, in \u001b[36m_BaseKMeans._check_params_vs_input\u001b[39m\u001b[34m(self, X, default_n_init)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, default_n_init=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# n_clusters\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m X.shape[\u001b[32m0\u001b[39m] < \u001b[38;5;28mself\u001b[39m.n_clusters:\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    872\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m should be >= n_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    873\u001b[39m         )\n\u001b[32m    875\u001b[39m     \u001b[38;5;66;03m# tol\u001b[39;00m\n\u001b[32m    876\u001b[39m     \u001b[38;5;28mself\u001b[39m._tol = _tolerance(X, \u001b[38;5;28mself\u001b[39m.tol)\n",
      "\u001b[31mValueError\u001b[39m: n_samples=4 should be >= n_clusters=5."
     ]
    }
   ],
   "source": [
    "wss(embedding_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agglomerative method\n",
      "0: ['audio_snimci/2.wav', 'audio_snimci/4.wav']\n",
      "1: ['audio_snimci/1.wav']\n",
      "2: ['audio_snimci/3.wav']\n",
      "K Means fixed method\n",
      "0: ['audio_snimci/3.wav']\n",
      "1: ['audio_snimci/2.wav', 'audio_snimci/4.wav']\n",
      "2: ['audio_snimci/1.wav']\n",
      "K Means fixed method with Elbow Technique\n",
      "0: ['audio_snimci/2.wav', 'audio_snimci/4.wav']\n",
      "1: ['audio_snimci/1.wav', 'audio_snimci/3.wav']\n",
      "Mean Shift method\n",
      "0: ['audio_snimci/2.wav']\n",
      "1: ['audio_snimci/4.wav']\n",
      "2: ['audio_snimci/1.wav']\n",
      "3: ['audio_snimci/3.wav']\n",
      "My algorithm\n",
      "0: ['audio_snimci/1.wav', 'audio_snimci/3.wav']\n",
      "1: ['audio_snimci/2.wav', 'audio_snimci/4.wav']\n"
     ]
    }
   ],
   "source": [
    "agglomerative(list_files_new,embedding_new,3)\n",
    "print(\"K Means fixed method\")\n",
    "k_means_fixed(list_files_new,embedding_new,3)\n",
    "print(\"K Means fixed method with Elbow Technique\")\n",
    "k_means_fixed(list_files_new,embedding_new,2)\n",
    "mean_shift(list_files_new,embedding_new)\n",
    "my_algorithm_v2(list_files_new,embedding_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
