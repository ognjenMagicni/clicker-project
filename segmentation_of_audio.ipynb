{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "file_path_audio = \"test_12.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_audio_segment(input_path: str, start_sec: float, end_sec: float, output_path: str = None) -> AudioSegment:\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    start_ms = int(start_sec * 1000)\n",
    "    end_ms = int(end_sec * 1000)\n",
    "    cut_audio = audio[start_ms:end_ms]\n",
    "    if output_path:\n",
    "        cut_audio.export(output_path, format=\"mp3\")\n",
    "\n",
    "    return cut_audio\n",
    "\n",
    "def function_allocate_with_successive(list_timestamp):\n",
    "  os.mkdir('speakers')\n",
    "  unique_elements = set([ f[2] for f in list_timestamp])\n",
    "  no_speakers = len(unique_elements)\n",
    "  print(f\"Number of speakers: {no_speakers}\")\n",
    "  for speaker in unique_elements:\n",
    "    os.mkdir('speakers/'+speaker)\n",
    "\n",
    "  start = 0\n",
    "  end = 0\n",
    "  previous_speaker = None\n",
    "  for timestamp in list_timestamp:\n",
    "    duration = timestamp[1]-timestamp[0]\n",
    "    if duration<2:\n",
    "      continue\n",
    "    if previous_speaker == timestamp[2]:\n",
    "      end = timestamp[1]\n",
    "    else:\n",
    "      if previous_speaker:\n",
    "        cut_audio_segment('all_audios.mp3',start,end,'speakers/'+previous_speaker+'/'+f\"{start:.1f}_{end:.1f}.mp3\")\n",
    "      start = timestamp[0]\n",
    "      end = timestamp[1]\n",
    "      previous_speaker = timestamp[2]\n",
    "  cut_audio_segment('all_audios.mp3',start,end,'speakers/'+previous_speaker+'/'+f\"{start:.1f}_{end:.1f}.mp3\")\n",
    "\n",
    "def function_allocate(list_timestamp, audio_path):\n",
    "  os.mkdir('speakers')\n",
    "  unique_elements = set([ f[2] for f in list_timestamp])\n",
    "  no_speakers = len(unique_elements)\n",
    "  print(f\"Number of speakers: {no_speakers}\")\n",
    "  for speaker in unique_elements:\n",
    "    os.mkdir('speakers/'+speaker)\n",
    "\n",
    "\n",
    "  for timestamp in list_timestamp:\n",
    "    start = timestamp[0]\n",
    "    end = timestamp[1]\n",
    "    speaker = timestamp[2]\n",
    "\n",
    "    duration = timestamp[1]-timestamp[0]\n",
    "    if duration<1:\n",
    "      continue\n",
    "\n",
    "    cut_audio_segment(audio_path,start,end,'speakers/'+speaker+'/'+f\"{start:.1f}_{end:.1f}.mp3\")\n",
    "\n",
    "def function_list_timestamp(audio_path):\n",
    "  pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=token)\n",
    "\n",
    "  pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "  diarization = pipeline(audio_path)\n",
    "\n",
    "  list_timestamp = []\n",
    "  for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "      #print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "      list_timestamp.append((turn.start,turn.end,speaker))\n",
    "  return list_timestamp\n",
    "\n",
    "def present_list_timestamps(list_timestamp):\n",
    "  for f in list_timestamp:\n",
    "    if f[1]-f[0]>=1:\n",
    "      m1 = str(int(f[0]/60))\n",
    "      s1 = str(int(f[0]%60))\n",
    "      m2 = str(int(f[1]/60))\n",
    "      s2 = str(int(f[1]%60))\n",
    "      first_t = m1+\":\"+s1 if int(s1)>=10 else m1+\":\"+\"0\"+s1\n",
    "      second_t = m2+\":\"+s2 if int(s2)>=10 else m2+\":\"+\"0\"+s2\n",
    "      print((first_t,second_t,f[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/Desktop/Ognjen/ClickerProject/venv/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0:00', '0:01', 'SPEAKER_00')\n",
      "('0:01', '0:03', 'SPEAKER_02')\n",
      "('0:03', '0:06', 'SPEAKER_00')\n",
      "('0:05', '0:08', 'SPEAKER_02')\n",
      "('0:08', '0:11', 'SPEAKER_00')\n",
      "('0:11', '0:12', 'SPEAKER_01')\n",
      "('0:13', '0:14', 'SPEAKER_02')\n",
      "Number of speakers: 3\n"
     ]
    }
   ],
   "source": [
    "list_timestamp = function_list_timestamp(file_path_audio)\n",
    "present_list_timestamps(list_timestamp)\n",
    "function_allocate(list_timestamp,file_path_audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
